{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/25 01:11:11 WARN Utils: Your hostname, codespaces-a019c6 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "23/11/25 01:11:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/25 01:11:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_records = '../data/records/'\n",
    "input_loc = '../data/loc/'\n",
    "df_records = spark.read.parquet(input_records)\n",
    "df_loc = spark.read.parquet(input_loc)\n",
    "\n",
    "df_records.createOrReplaceTempView('records')\n",
    "df_loc.createOrReplaceTempView('loc')\n",
    "\n",
    "df_result = spark.sql(\"\"\"\n",
    "select    SNo, \n",
    "          TO_DATE(ObservationDate, 'MM/dd/yyyy') as ObservationDate, \n",
    "          r.`Province/State`, \n",
    "          r.`Country/Region`,\n",
    "          CAST(Confirmed as INTEGER) as Confirmed, \n",
    "          CAST(Deaths as INTEGER) as Deaths, \n",
    "          CAST(Recovered as INTEGER) as Recovered, \n",
    "          CAST(Lat as FLOAT) as Lat, \n",
    "          CAST(Long as FLOAT) as Long\n",
    "          from records r left join loc l\n",
    "          on r.`Province/State` = l.`Province/State`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.createOrReplaceTempView('covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sql(\"\"\"\n",
    "select `Province/State`, max(Confirmed) as FinalConfirmed, max(Deaths) as FinalDeaths, max(Recovered) as FinalRecovered, max(Lat) as Lat, max(Long) as Long\n",
    "from covid\n",
    "where ObservationDate='2021-05-29' and `Province/State`!='Unknown'\n",
    "group by `Province/State`\n",
    "order by `Province/State`\n",
    "          \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Province/State: string (nullable = true)\n",
      " |-- FinalConfirmed: integer (nullable = true)\n",
      " |-- FinalDeaths: integer (nullable = true)\n",
      " |-- FinalRecovered: integer (nullable = true)\n",
      " |-- Lat: float (nullable = true)\n",
      " |-- Long: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "assembler = VectorAssembler(inputCols=['Lat', 'Long'], outputCol='features')\n",
    "df_assembled = assembler.transform(data)\n",
    "df = df_assembled.select('features', 'FinalConfirmed')\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(regParam=0.2, labelCol='FinalConfirmed')\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------------------+\n",
      "|FinalConfirmed|        prediction|            features|\n",
      "+--------------+------------------+--------------------+\n",
      "|          1059|2968.0020392221277|[40.1823997497558...|\n",
      "|           194|3199.1471049302363|[35.7518005371093...|\n",
      "|           616|3340.8171580007233|[35.1916999816894...|\n",
      "|           253| 3159.183641298928|[37.5777015686035...|\n",
      "|          1025|3696.4332189245156|[30.6170997619628...|\n",
      "|           393|3074.5913284561007|[39.3054008483886...|\n",
      "|             1|3317.4813157284825|[31.6926994323730...|\n",
      "+--------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "predictions.select('FinalConfirmed', 'prediction', 'features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 2774.173380695434\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='FinalConfirmed', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
