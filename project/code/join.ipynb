{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/24 22:16:45 WARN Utils: Your hostname, codespaces-a019c6 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "23/11/24 22:16:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/24 22:16:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_records = '../data/records/'\n",
    "input_loc = '../data/loc/'\n",
    "df_records = spark.read.parquet(input_records)\n",
    "df_loc = spark.read.parquet(input_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_records.createOrReplaceTempView('records')\n",
    "df_loc.createOrReplaceTempView('loc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SNo: integer (nullable = true)\n",
      " |-- ObservationDate: string (nullable = true)\n",
      " |-- Province/State: string (nullable = true)\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Last Update: string (nullable = true)\n",
      " |-- Confirmed: float (nullable = true)\n",
      " |-- Deaths: float (nullable = true)\n",
      " |-- Recovered: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_records.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Province/State: string (nullable = true)\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Long: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_loc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+--------------+--------------+---------+------+---------+-------+--------+\n",
      "| SNo|ObservationDate|Province/State|Country/Region|Confirmed|Deaths|Recovered|    Lat|    Long|\n",
      "+----+---------------+--------------+--------------+---------+------+---------+-------+--------+\n",
      "|   1|     2020-01-22|         Anhui|Mainland China|        1|     0|        0|31.8257|117.2264|\n",
      "|  41|     2020-01-23|         Anhui|Mainland China|        9|     0|        0|31.8257|117.2264|\n",
      "| 100|     2020-01-24|         Anhui|Mainland China|       15|     0|        0|31.8257|117.2264|\n",
      "| 138|     2020-01-25|         Anhui|Mainland China|       39|     0|        0|31.8257|117.2264|\n",
      "| 185|     2020-01-26|         Anhui|Mainland China|       60|     0|        0|31.8257|117.2264|\n",
      "| 236|     2020-01-27|         Anhui|Mainland China|       70|     0|        0|31.8257|117.2264|\n",
      "| 287|     2020-01-28|         Anhui|Mainland China|      106|     0|        0|31.8257|117.2264|\n",
      "| 339|     2020-01-29|         Anhui|Mainland China|      152|     0|        2|31.8257|117.2264|\n",
      "| 395|     2020-01-30|         Anhui|Mainland China|      200|     0|        2|31.8257|117.2264|\n",
      "| 456|     2020-01-31|         Anhui|Mainland China|      237|     0|        3|31.8257|117.2264|\n",
      "| 519|     2020-02-01|         Anhui|Mainland China|      297|     0|        5|31.8257|117.2264|\n",
      "| 588|     2020-02-02|         Anhui|Mainland China|      340|     0|        7|31.8257|117.2264|\n",
      "| 657|     2020-02-03|         Anhui|Mainland China|      408|     0|       14|31.8257|117.2264|\n",
      "| 727|     2020-02-04|         Anhui|Mainland China|      480|     0|       20|31.8257|117.2264|\n",
      "| 800|     2020-02-05|         Anhui|Mainland China|      530|     0|       23|31.8257|117.2264|\n",
      "| 873|     2020-02-06|         Anhui|Mainland China|      591|     0|       34|31.8257|117.2264|\n",
      "| 945|     2020-02-07|         Anhui|Mainland China|      665|     0|       47|31.8257|117.2264|\n",
      "|1019|     2020-02-08|         Anhui|Mainland China|      733|     0|       59|31.8257|117.2264|\n",
      "|1093|     2020-02-09|         Anhui|Mainland China|      779|     1|       72|31.8257|117.2264|\n",
      "|1167|     2020-02-10|         Anhui|Mainland China|      830|     3|       88|31.8257|117.2264|\n",
      "+----+---------------+--------------+--------------+---------+------+---------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select    SNo, \n",
    "          TO_DATE(ObservationDate, 'MM/dd/yyyy') as ObservationDate, \n",
    "          r.`Province/State`, \n",
    "          r.`Country/Region`,\n",
    "          CAST(Confirmed as INTEGER) as Confirmed, \n",
    "          CAST(Deaths as INTEGER) as Deaths, \n",
    "          CAST(Recovered as INTEGER) as Recovered, \n",
    "          CAST(Lat as FLOAT) as Lat, \n",
    "          CAST(Long as FLOAT) as Long\n",
    "          from records r left join loc l\n",
    "          on r.`Province/State` = l.`Province/State`\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
